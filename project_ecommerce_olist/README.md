# üóÉÔ∏è Brazilian ecommerce olist

Ap√≥s ingerirmos os dados do Kaggle, iremos fazer transforma√ß√µes e responder algumas perguntas de neg√≥cio, como:
- 1: Quantas vendas ocorrem por estado?
- 2: Qual loja/cidade captou o maior valor em vendas?
  - 2.1: Quanto desse valor veio por valor do produto?
  - 2.2: Quando desse valor foi por frete?
- 3: Qual a rela√ß√£o entre o dia de compra e o atraso na entrega?
- 4: Qual a rela√ß√£o entre o tempo da compra at√© o envio com o tempo total at√© a entrega?
- 5: Quais s√£o os maiores compradores? (rec√™ncia, frequ√™ncia e valor gasto)

Essas e outras tantas perguntas podem ser respondidas com os dados que trataremos.

Juntem-se a mim, enquanto fazemos um overview do projeto.

Este √© um conjunto de dados p√∫blicos de com√©rcio eletr√¥nico brasileiro das compras feitas na loja Olist. O conjunto de dados cont√©m informa√ß√µes de 100 mil pedidos de 2016 a 2018 feitos em v√°rios marketplaces no Brasil. Suas caracter√≠sticas permitem visualizar um pedido em v√°rias dimens√µes: desde o status do pedido, pre√ßo, pagamento e desempenho de frete at√© a localiza√ß√£o do cliente, atributos do produto e, finalmente, avalia√ß√µes escritas pelos clientes. Tamb√©m disponibilizamos um conjunto de dados de geolocaliza√ß√£o que relaciona os c√≥digos postais brasileiros √†s coordenadas lat/long.

Estes s√£o dados comerciais reais, foram anonimizados, e as refer√™ncias √†s empresas e parceiros no texto de revis√£o foram substitu√≠das pelos nomes das grandes casas de Game of Thrones.

## 0 - beginning

Antes de tudo, executamos um script para cria√ß√£o dos databases (bronze, silver e gold)

## 1 - Ingest√£o de dados (staging)
O arquivo .ipynb respons√°vel pela ingest√£o pode ser visto [aqui](https://github.com/gustavocrod/databricks-data-engineering-olist/blob/main/0%20-%20data_ingestion%20(staging).ipynb)

O dataset escolhido foi o [Brazilian E-Commerce Public Dataset by Olist](https://www.kaggle.com/datasets/olistbr/brazilian-ecommerces)
Utilizamos do opendatasets para fazer download diretamente do kaggle, ao adicionar as credenciais em um arquivo chamado kaggle.json na raiz do projeto (arquivo ignorado pelo .gitignore).

Como √© um dataset est√°tico (ou quase 100%), n√£o faz sentido adicionar upsert e tampouco streaming.

Mas aqui poder√≠amos utilizar do AutoLoader, ou at√© mesmo de alguma ferramenta com CDC, como Airbyte.

Nossa staging n√£o precisaria existir (apenas caso fossem dados vindos por airbyte, por exemplo). Mas criamos para exemplificar, pois irei salvar a staging em parquet. Depois disso, todas as camadas ser√£o em Delta.

# Delta Lake House
Workflow
![workflows](extra/workflows.PNG)

Agendamento
![agendamento](extra/agendamento.PNG)
____
## [ü•â Bronze](https://github.com/gustavocrod/databricks-data-engineering/tree/main/project_ecommerce_olist/1%20-%20Bronze)

**Camada inicial, dados _as is_**

Muito importante que dados nessa camada reflitam o banco ou fonte dos dados

aqui podemos ter duplicidade em vers√µes de dados que devem ser tratados nas camadas posteriores.
Costumo chamar essa camada de "lake"

### 1 - Processamento da camada bronze

Aqui adicionamos uma estrutura que permite um la√ßo de repeti√ß√£o.
O la√ßo ser√° respons√°vel por armazenar os dados e criar tabela delta referente a cada "entidade" definida no diagrama ER
![diagrama er](extra/er_olist.png)


### 2 - Persist√™ncia

Estamos pegando os arquivos em parquet (passo apenas did√°tico), salvando os dados em delta e criando as tabelas delta

____

## [ü•à Silver]((https://github.com/gustavocrod/databricks-data-engineering/tree/main/project_ecommerce_olist/2%20-%20Silver))

**camada de limpeza, normaliza√ß√£o e enriquecimento de dados.**

e.g., 
 - uppercase
 - data textual para tipo date
 - dias em atraso (diferen√ßa entre data atual e data de envimento)

p.s. embora tenha visto v√°rias implementa√ß√µes distintas em projetos que atuei, prefiro desconsiderar as regras de neg√≥cio nessa camada (deixamos para aplicar na camada gold)
Na camada silver, limpezas e ajustes em dados devem ser aplicados
Caso seja poss√≠vel, enriquecer os dados e extrair dados tamb√©m deve acontecer nessa camada (defini√ß√£o adotada)

----
## [ü•á Gold]((https://github.com/gustavocrod/databricks-data-engineering-olist/tree/main/3%20-%20Gold))

**camada para aplica√ß√£o de regras de neg√≥cio**

e.g.,
 - jun√ß√£o/uni√£o de tabelas
 - filtro de dados

 Nessa camada aplicamos dois tipos de agrega√ß√µes:
  * **1 - agrega√ß√£o _estilo_ dimens√£o e fato.** _i.e._, adicionamos joins entre as tabelas, conforme o schema disponibilizado.
  Dessa forma, como a tabela cont√©m v√°rios dados, multiplas agrega√ß√µes s√£o podem ser feitas ao conectar essa tabela diretamente em ferramentes de visualiza√ß√£o como o Power BI e Metabase, ou at√© mesmo via databricks Dashboards. Sendo ela como uma esp√©cie de data mart
  * **2 - agrega√ß√£o anal√≠tica**. _i.e._, sumariza√ß√£o de dados.
  Dessa forma podemos adicionar em ferramentas mais simples ou tamb√©m √© util para algum analista que n√£o det√©m conhecimento em SQL.

___

### üìú Caso de estudo RFV

  Para responder as quest√µes levantadas, tivemos um trabalho extra para explicitar a quest√£o **5 Quais s√£o os maiores compradores?**. Aqui avan√ßamos para montar a base par an√°lise de padr√£o de compras por clientes: **RFV**


RFV, ou Recency, Frequency, and Value, √© uma t√©cnica de an√°lise de dados frequentemente usada em marketing e gerenciamento de clientes para segmentar clientes com base em seu comportamento de compra.

Essa abordagem analisa tr√™s aspectos principais do comportamento do cliente:

- **Recency (Rec√™ncia):** Refere-se √† √∫ltima vez que um cliente fez uma compra. Geralmente, clientes que fizeram compras recentes s√£o mais propensos a fazer compras futuras do que aqueles que n√£o compraram h√° muito tempo.
- **Frequency (Frequ√™ncia):** Refere-se √† frequ√™ncia com que um cliente faz compras durante um determinado per√≠odo de tempo. Clientes que compram com frequ√™ncia podem ser considerados mais leais e valiosos para a empresa.
- **Value (Valor):** Refere-se ao valor monet√°rio total das compras feitas por um cliente durante um determinado per√≠odo de tempo. Clientes que gastam mais t√™m um valor de vida do cliente mais alto e podem ser alvos de estrat√©gias de marketing mais agressivas.

Ao analisar esses tr√™s aspectos juntos, as empresas podem segmentar seus clientes em diferentes grupos com base em seu comportamento de compra e adaptar suas estrat√©gias de marketing e relacionamento com o cliente de acordo. Por exemplo, clientes com alta rec√™ncia, frequ√™ncia e valor podem ser segmentados como clientes VIP e receber ofertas exclusivas, enquanto clientes com baixa rec√™ncia, frequ√™ncia e valor podem ser alvos de campanhas de reativa√ß√£o.

## Dashboard
![dash final](extra/dash.PNG)

Ao analisar o dashboard, podemos ver que em SP √© onde tem o maior numero de compradores (faz sentido pela popula√ß√£o).
Podemos ver tamb√©m as TOP5 cidades que mais venderam e os TOP5 clientes de mais compraram

Podemos observar tamb√©m a rela√ß√£o entre o tempo de despache e o tempo total da compra at√© recebimento da mercadoria. Embora tenha liga√ß√£o linear, podemos observar que v√°rios casos tem atraso mesmo quando o despache √© agil.

Por fim, observamos que compras feitas no final de semana possuem maior m√©dia de dias em atraso para a entrega.
