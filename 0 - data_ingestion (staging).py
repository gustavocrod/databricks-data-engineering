# Databricks notebook source
# MAGIC %md
# MAGIC
# MAGIC # üóÉÔ∏è Brazilian ecommerce olist
# MAGIC
# MAGIC Este √© um conjunto de dados p√∫blicos de com√©rcio eletr√¥nico brasileiro das compras feitas na loja Olist. O conjunto de dados cont√©m informa√ß√µes de 100 mil pedidos de 2016 a 2018 feitos em v√°rios marketplaces no Brasil. Suas caracter√≠sticas permitem visualizar um pedido em v√°rias dimens√µes: desde o status do pedido, pre√ßo, pagamento e desempenho de frete at√© a localiza√ß√£o do cliente, atributos do produto e, finalmente, avalia√ß√µes escritas pelos clientes. Tamb√©m disponibilizamos um conjunto de dados de geolocaliza√ß√£o que relaciona os c√≥digos postais brasileiros √†s coordenadas lat/long.
# MAGIC
# MAGIC Estes s√£o dados comerciais reais, foram anonimizados, e as refer√™ncias √†s empresas e parceiros no texto de revis√£o foram substitu√≠das pelos nomes das grandes casas de Game of Thrones.

# COMMAND ----------

# MAGIC %run ./_utils

# COMMAND ----------

# MAGIC %run ./_create_database

# COMMAND ----------

# MAGIC %md
# MAGIC
# MAGIC ## dependency

# COMMAND ----------

!pip install opendatasets

# COMMAND ----------

# MAGIC %md
# MAGIC
# MAGIC ## download dataset from kaggle
# MAGIC
# MAGIC O dataset escolhido foi o [Brazilian E-Commerce Public Dataset by Olist](https://www.kaggle.com/datasets/olistbr/brazilian-ecommerces)
# MAGIC
# MAGIC Como √© um dataset est√°tico (ou quase 100%), n√£o faz sentido adicionar upsert e tampouco streaming.
# MAGIC
# MAGIC Mas aqui poder√≠amos utilizar do AutoLoader, ou at√© mesmo de alguma ferramenta com CDC, como airbyte
# MAGIC
# MAGIC Nossa staging n√£o precisaria existir (apenas caso fossem dados vindos por airbyte, por exemplo). Mas vou criar aqui apenas para exemplificar, pois irei salvar a staging em parquet. Depois disso, todas as camadas ser√£o em Delta

# COMMAND ----------

import opendatasets as od
od.download("https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce", "dataset/")

# COMMAND ----------

# MAGIC %ls dbfs:/FileStore/dataset/brazilian-ecommerce/

# COMMAND ----------

# MAGIC %md
# MAGIC
# MAGIC os arquivos foram salvos em `/FileStore/dataset/brazilian-ecommerce/olist_*.csv``

# COMMAND ----------

# MAGIC %md
# MAGIC
# MAGIC ## Carregando o arquivo .csv para spark dataframe
# MAGIC
# MAGIC aqui o .od fez eu usar o workspace files. [mais detalhes aqui](https://docs.databricks.com/en/files/index.html)

# COMMAND ----------

df = (
    spark.read.format("csv")
    .options(header="true", inferSchema=True)
    .load("dataset/brazilian-ecommerce/olist_customers_dataset.csv")
)

# COMMAND ----------

# MAGIC %md
# MAGIC
# MAGIC ## üîé First exploratory data analysis (mini descriptive)

# COMMAND ----------

display(df)

# COMMAND ----------

print(f"Total de linhas: {df.count()}")

# COMMAND ----------

# MAGIC %md
# MAGIC
# MAGIC Data set central. Deve ser usado com o uma especie de "tabela fato"
# MAGIC ### schema
# MAGIC image

# COMMAND ----------

df.printSchema()

# COMMAND ----------

# MAGIC %md
# MAGIC
# MAGIC ## Saving data
# MAGIC
# MAGIC salvando todos os .csv no dbf em arquivo parquet
# MAGIC
# MAGIC Nossa staging poderia ser um S3 (aws) ou blob storage, mas vai ser diretamente no dbfs mesmo

# COMMAND ----------

files = [
    "olist_customers_dataset",
    "olist_orders_dataset",
    "olist_geolocation_dataset",
    "olist_products_dataset",
    "olist_order_items_dataset",
    "olist_sellers_dataset",
    "olist_order_payments_dataset",
    "product_category_name_translation",
    "olist_order_reviews_dataset",
]

# COMMAND ----------

for file_name in files:
    # multiline √© preciso por conta principalmente dos reviews. pois tem  \n
    df = (
        spark.read.format("csv")
        .options(header="true", inferSchema=True, multiLine=True, quote='"', escape='"')
        .load(f"dbfs:/FileStore/dataset/brazilian-ecommerce/{file_name}.csv")
    )

    save_dataframe(
        df,
        format_mode="parquet",
        target_location=f"dbfs:/FileStore/parquet/brazilian_ecommerce/{file_name}",
    )

# COMMAND ----------

# MAGIC %md
# MAGIC
# MAGIC Aqui utilizamos de algumas options para leitura do csv
# MAGIC
# MAGIC ```
# MAGIC .option("quote", "\"") # para que o delimitador de string seja "
# MAGIC .option("header", True) # para que a primeira linha do csv seja lida como header
# MAGIC .option("inferSchema", True) # para que infira o schema automaticamente
# MAGIC .option("multiLine", True) # para casos onde tem quebra de linha no texto
# MAGIC .option("escape", "\"") # para ignorar aspas duplas dentro de uma string (principalmente nos reviews)
# MAGIC
# MAGIC ```

# COMMAND ----------

# MAGIC %fs ls '/FileStore/parquet/brazilian_ecommerce/'

# COMMAND ----------

dbutils.notebook.exit("OK")

# COMMAND ----------

# MAGIC %md
# MAGIC
# MAGIC #### test

# COMMAND ----------

display(spark.read.parquet('/FileStore/parquet/brazilian_ecommerce/olist_order_reviews_dataset/'))

# COMMAND ----------


